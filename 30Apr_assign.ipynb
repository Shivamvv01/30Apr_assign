{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4fdde41-87e8-4d51-9541-b00de97ad4f3",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of homogeneity and completeness in clustering evaluation. How are they\n",
    "calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a4fa41-2154-4553-a4c2-58531268c6a6",
   "metadata": {},
   "source": [
    "Homogeneity and completeness are two metrics used to evaluate the quality of clustering results, primarily in scenarios where you have ground truth labels available for your data. These metrics help assess the extent to which the clusters created by a clustering algorithm match the true classes or labels in the data.\n",
    "\n",
    "Homogeneity:\n",
    "\n",
    "Homogeneity measures the extent to which each cluster contains only data points that belong to a single class or category. In other words, it assesses whether the clusters are pure with respect to the true class labels.\n",
    "A high homogeneity score indicates that the clusters are highly pure, with data points of the same class mostly grouped together.\n",
    "The homogeneity score ranges from 0 to 1, where 1 represents perfect homogeneity.\n",
    "Mathematical Formula for Homogeneity:\n",
    "\n",
    "Let (C) be the set of clusters created by the clustering algorithm.\n",
    "\n",
    "Let (T) be the set of true class labels.\n",
    "\n",
    "The homogeneity score (H) is calculated as follows:\n",
    "\n",
    "H(C, T) = 1 - {H(C | T)} / {H(T)}\n",
    "\n",
    "Where:\n",
    "\n",
    "(H(C | T)) is the conditional entropy of the clusters given the true class labels.\n",
    "(H(T)) is the entropy of the true class labels.\n",
    "Completeness:\n",
    "\n",
    "Completeness measures the extent to which all data points that belong to the same class are assigned to the same cluster. It assesses whether all data points of the same class are grouped together in one or more clusters.\n",
    "A high completeness score indicates that the clustering has captured all data points of the same class well.\n",
    "Like homogeneity, the completeness score also ranges from 0 to 1, where 1 represents perfect completeness.\n",
    "Mathematical Formula for Completeness:\n",
    "\n",
    "Completeness (C) is calculated as follows:\n",
    "\n",
    "C(C, T) = 1 - {H(T | C)} / {H(T)}\n",
    "\n",
    "Where:\n",
    "\n",
    "(H(T | C)) is the conditional entropy of the true class labels given the clusters.\n",
    "(H(T)) is the entropy of the true class labels.\n",
    "These two metrics are complementary and can be used together to provide a more comprehensive evaluation of clustering results. High homogeneity and completeness scores suggest that the clustering has done a good job of grouping data points that belong to the same classes.\n",
    "\n",
    "It's important to note that while these metrics are useful when ground truth labels are available, they may not be applicable in unsupervised scenarios where true class labels are unknown. In such cases, other metrics like silhouette score or Davies-Bouldin index may be more appropriate for evaluating clustering performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cc0f60-6cd6-4415-82c5-ff65c4058c9c",
   "metadata": {},
   "source": [
    "Q2. What is the V-measure in clustering evaluation? How is it related to homogeneity and completeness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824f97a-22e2-48c0-b306-19d0b7c1301f",
   "metadata": {},
   "source": [
    "The V-measure is another clustering evaluation metric that combines the concepts of homogeneity and completeness to provide a single measure of the overall quality of clustering results. It is particularly useful when you want a single metric that balances both aspects of clustering performance.\n",
    "\n",
    "The V-measure is defined as the harmonic mean of homogeneity (H) and completeness (C), and it is calculated as follows:\n",
    "\n",
    "V = (2 * H * C) / (H + C)\n",
    "\n",
    "Where:\n",
    "\n",
    "(H) is the homogeneity score.\n",
    "(C) is the completeness score.\n",
    "The V-measure ranges from 0 to 1, with 1 indicating perfect clustering, where all data points of the same class are grouped together in the same cluster (high homogeneity) and all data points within a class are assigned to the same cluster (high completeness).\n",
    "\n",
    "In summary, the V-measure combines homogeneity and completeness into a single metric, providing a balanced assessment of clustering quality. It is useful when you want a single score to evaluate how well a clustering algorithm has grouped data points of the same class together while ensuring that all data points within a class are assigned to the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76063d-7292-4c13-98e2-cbf26a42ff14",
   "metadata": {},
   "source": [
    "Q3. How is the Silhouette Coefficient used to evaluate the quality of a clustering result? What is the range\n",
    "of its values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba4703-3823-46d9-9653-b1e3f810c04e",
   "metadata": {},
   "source": [
    "The Silhouette Coefficient is a metric used to evaluate the quality of a clustering result, providing an indication of how well-separated the clusters are. It measures both the cohesion (how close data points are to members of the same cluster) and separation (how far apart data points are from members of other clusters) of the clusters. The higher the Silhouette Coefficient, the better the clustering.\n",
    "\n",
    "The Silhouette Coefficient ranges from -1 to 1:\n",
    "A high value (close to 1) indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters, suggesting a good clustering.\n",
    "A value near 0 indicates that the object is on or very close to the decision boundary between two neighboring clusters.\n",
    "A low value (close to -1) indicates that the object is poorly matched to its own cluster and well matched to neighboring clusters, suggesting that it may belong to the wrong cluster.\n",
    "In summary, the Silhouette Coefficient measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). A higher Silhouette Coefficient indicates better clustering quality, and values near 0 suggest overlapping clusters or data points near cluster boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628bfe27-3179-4736-8f51-31b1d56a6b39",
   "metadata": {},
   "source": [
    "Q4. How is the Davies-Bouldin Index used to evaluate the quality of a clustering result? What is the range\n",
    "of its values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a5c86-27dc-46fd-8b0b-4c1d91ad59df",
   "metadata": {},
   "source": [
    "The Davies-Bouldin Index (DBI) is a metric used to evaluate the quality of a clustering result. It measures the average similarity between each cluster and the cluster that is most similar to it, where similarity is defined as the ratio of the within-cluster dispersion (intra-cluster distance) to the between-cluster dispersion (inter-cluster distance). The goal is to find clusters that are well-separated from each other and have low intra-cluster variance.\n",
    "\n",
    "DBI Ranges from 0 to positive integers :\n",
    "A small DBI value indicates that the clusters are well-separated and have low intra-cluster variance relative to inter-cluster variance.\n",
    "\n",
    "A larger DBI value suggests that the clusters are not well-separated, and the clustering may not be of high quality.\n",
    "\n",
    "The range of DBI values depends on the dataset and clustering results, but in practice, it typically falls within the range of 0 to positive values, where lower values are preferred. However, there is no strict upper bound for the DBI, as it depends on the dataset characteristics and the quality of clustering.\n",
    "\n",
    "In summary, the Davies-Bouldin Index quantifies the average similarity between each cluster and the cluster most similar to it, providing a measure of cluster separation and quality. Lower DBI values indicate better clustering quality, while higher values suggest less distinct and more overlapping clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5b9a2-846d-4151-a0ab-ac19638ef61a",
   "metadata": {},
   "source": [
    "Q5. Can a clustering result have a high homogeneity but low completeness? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e340db49-967e-457b-a48f-d1b96df8b8e2",
   "metadata": {},
   "source": [
    "Yes, a clustering result can have high homogeneity but low completeness. This situation often arises when the clustering algorithm produces highly pure clusters but fails to assign all data points of a particular class to a single cluster.\n",
    "\n",
    "Example,\n",
    "\n",
    "Imagine we have a dataset of various fruits, including apples, bananas, and oranges, and we want to perform clustering. Our dataset consists of 100 samples, with the following distribution:\n",
    "\n",
    "60 apple samples\n",
    "30 banana samples\n",
    "10 orange samples\n",
    "Now, let's say we apply a clustering algorithm that produces the following clusters:\n",
    "\n",
    "Cluster 1: 60 samples (all apples) Cluster 2: 20 samples (10 bananas and 10 oranges)\n",
    "\n",
    "In this clustering result:\n",
    "\n",
    "Homogeneity: The homogeneity measures how pure each cluster is in terms of containing samples from a single class. Cluster 1 is entirely composed of apples, so it is highly pure, resulting in high homogeneity.\n",
    "\n",
    "Completeness: Completeness measures how well each class is assigned to a single cluster. While Cluster 1 contains all the apple samples, Cluster 2 combines both banana and orange samples. As a result, not all samples of the banana and orange classes are assigned to a single cluster, leading to low completeness.\n",
    "\n",
    "So, in this example, the clustering result exhibits high homogeneity (pure clusters) but low completeness (classes are not entirely assigned to a single cluster). This scenario is common when the clustering algorithm emphasizes cluster purity but does not ensure that all samples of each class are grouped together into one cluster. Both homogeneity and completeness are important aspects to consider when assessing the quality of a clustering result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8551ffb-a129-4a49-b211-b8ab7c9c198d",
   "metadata": {},
   "source": [
    "Q6. How can the V-measure be used to determine the optimal number of clusters in a clustering\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27be00-7d80-47e1-9ba9-2968d712ba25",
   "metadata": {},
   "source": [
    "The V-Measure is a clustering evaluation metric that combines both homogeneity and completeness to provide a single score that quantifies the overall quality of a clustering result. While it can be used to assess the quality of a clustering result, it is not typically used to directly determine the optimal number of clusters. Instead, the V-Measure is used after clustering to evaluate how well the clusters align with the ground truth (if available) or to compare different clustering results.\n",
    "\n",
    "To determine the optimal number of clusters in a clustering algorithm, you would typically use other methods or metrics, such as the following:\n",
    "\n",
    "Elbow Method: Plot the clustering score (e.g., inertia or within-cluster sum of squares) as a function of the number of clusters. The \"elbow point\" in the plot, where the score starts to level off, is often considered a good estimate for the optimal number of clusters.\n",
    "\n",
    "Silhouette Score: Calculate the silhouette score for different numbers of clusters and choose the number of clusters that maximizes this score. A higher silhouette score indicates better-defined clusters.\n",
    "\n",
    "Gap Statistics: Compare the clustering result's performance to that of a random clustering. If the clustering result significantly outperforms random clustering, it suggests that the number of clusters is meaningful.\n",
    "\n",
    "Davies-Bouldin Index: Minimize this index by trying different numbers of clusters. Lower values indicate better clustering.\n",
    "\n",
    "Visual Inspection: Visualize the data and clustering results using techniques like scatter plots or dendrograms (for hierarchical clustering). Look for a number of clusters that makes sense and aligns with your domain knowledge.\n",
    "\n",
    "Once you have determined the optimal number of clusters using one of these methods, you can then use the V-Measure or other evaluation metrics to assess the quality of the clustering with that specific number of clusters. The V-Measure provides a more comprehensive evaluation of the clustering result, considering both homogeneity and completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377552fd-fd12-413a-9b0a-c86798a5e705",
   "metadata": {},
   "source": [
    "Q7. What are some advantages and disadvantages of using the Silhouette Coefficient to evaluate a\n",
    "clustering result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d099df52-9b23-4786-8b42-a3247f5a2616",
   "metadata": {},
   "source": [
    "The Silhouette Coefficient is a metric used to evaluate the quality of a clustering result, providing an indication of how well-separated the clusters are.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Intuitive Interpretation: The Silhouette Coefficient is relatively easy to understand. It provides a measure of how similar an object is to its own cluster compared to other clusters, with values ranging from -1 (incorrect clustering) to +1 (high-quality clustering).\n",
    "\n",
    "No Assumptions About Cluster Shape: Unlike some other metrics, such as inertia (used in the elbow method for K-means), the Silhouette Coefficient does not assume that clusters are spherical or have a specific shape. It measures the cohesion and separation of points in a cluster without making strong assumptions about the data distribution.\n",
    "\n",
    "Applicability to Various Algorithms: The Silhouette Coefficient can be used with a wide range of clustering algorithms, making it versatile for evaluating different types of clustering methods.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Sensitivity to Distance Metric: The Silhouette Coefficient's performance can be sensitive to the choice of distance metric. Different distance metrics may yield different silhouette scores for the same data, which can make it challenging to compare clustering results across datasets or algorithms.\n",
    "\n",
    "Does Not Consider Global Structure: The Silhouette Coefficient provides a local measure of cluster quality for individual data points but does not consider the global structure of clusters. It may not detect issues like overlapping clusters or hierarchical relationships between clusters.\n",
    "\n",
    "Assumes Euclidean Distance: The Silhouette Coefficient is most commonly used with Euclidean distance, which may not be suitable for all types of data (e.g., categorical data or data with complex relationships). When using non-Euclidean distance metrics, the interpretation of silhouette scores can be less straightforward.\n",
    "\n",
    "May Not Reflect Domain-Specific Goals: The Silhouette Coefficient is a generic metric and may not always align with the specific goals or requirements of a particular clustering task. It does not consider domain-specific knowledge or constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98401e1c-8083-4f0f-89a3-db43a3277209",
   "metadata": {},
   "source": [
    "Q8. What are some limitations of the Davies-Bouldin Index as a clustering evaluation metric? How can\n",
    "they be overcome?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891427d-e5c8-48d3-b9fe-6b583ed0fa07",
   "metadata": {},
   "source": [
    "The Davies-Bouldin Index (DBI) is a clustering evaluation metric that measures the average similarity between each cluster and its most similar cluster.\n",
    "\n",
    "Limitations of the Davies-Bouldin Index (DBI):\n",
    "\n",
    "Sensitivity to the Number of Clusters: DBI tends to favor solutions with a larger number of clusters because as the number of clusters increases, the likelihood of finding clusters with low intra-cluster distances increases. This can lead to a bias towards over-segmentation.\n",
    "\n",
    "Assumes Spherical Clusters: DBI assumes that clusters are spherical and equally sized, which may not hold in many real-world datasets where clusters can have complex shapes and varying sizes. This assumption can lead to suboptimal results when clusters deviate significantly from this ideal.\n",
    "\n",
    "Lack of Normalization: DBI does not provide a normalized score, making it difficult to compare clustering results across datasets with different characteristics. A lower DBI score does not necessarily indicate better clustering; it only indicates a relative measure within the dataset.\n",
    "\n",
    "Dependence on Distance Metric: Like many clustering metrics, DBI's performance is sensitive to the choice of distance metric. Different distance metrics can yield different DBI scores for the same data, making comparisons problematic.\n",
    "\n",
    "Methods to Overcoming Limitations:\n",
    "\n",
    "Normalization: To address the lack of normalization, you can normalize the DBI score by dividing it by the average DBI score of a set of random clusters. This normalized score, called the Normalized Davies-Bouldin Index (NDBI), provides a more interpretable and comparable measure of cluster quality.\n",
    "\n",
    "Use Multiple Metrics: Rather than relying solely on DBI, consider using multiple clustering evaluation metrics in combination. Metrics like Silhouette Score, Adjusted Rand Index (ARI), or Normalized Mutual Information (NMI) can provide complementary insights into clustering quality, helping to overcome DBI's limitations.\n",
    "\n",
    "Visualization: Visualize the clustering results to gain a better understanding of the clusters' shapes, sizes, and inter-cluster relationships. This can help identify situations where DBI might not provide an accurate assessment of clustering quality.\n",
    "\n",
    "Domain Knowledge: Incorporate domain knowledge when evaluating clustering results. Sometimes, clusters may make sense from a domain-specific perspective, even if they do not achieve the lowest DBI score. Domain experts can provide valuable insights into the quality of clustering.\n",
    "\n",
    "Experiment with Different Distance Metrics: Since DBI is sensitive to the choice of distance metric, experiment with different distance metrics to find the one that best aligns with the data's characteristics and the problem's requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79909a-6f84-4edb-ba8b-0b703cd3c2d7",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between homogeneity, completeness, and the V-measure? Can they have\n",
    "different values for the same clustering result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05af3bc-e7a3-48bb-8534-ad99337e1c0f",
   "metadata": {},
   "source": [
    "Homogeneity, completeness, and the V-measure are three clustering evaluation metrics that measure different aspects of clustering quality. They are related but capture distinct characteristics of a clustering result:\n",
    "\n",
    "Homogeneity: Homogeneity measures how pure each cluster is, meaning that all data points in a cluster belong to the same true class or category. It quantifies whether clusters contain predominantly data points from a single class. Homogeneity is a value between 0 and 1, with higher values indicating better homogeneity.\n",
    "\n",
    "Completeness: Completeness measures whether all data points that belong to the same true class are assigned to the same cluster. It quantifies whether all data points of a given class are well-represented within a single cluster. Completeness is also a value between 0 and 1, with higher values indicating better completeness.\n",
    "\n",
    "V-measure: The V-measure is a metric that combines both homogeneity and completeness to provide a single score that represents the balance between them. It is the harmonic mean of homogeneity and completeness, given by the formula:\n",
    "\n",
    "V = 2 * (homogeneity * completeness) / (homogeneity + completeness)\n",
    "\n",
    "The V-measure takes values between 0 and 1, where a higher V-measure indicates better clustering quality. It is a useful metric when you want to consider both the purity of clusters (homogeneity) and the representation of true classes within clusters (completeness) simultaneously.\n",
    "\n",
    "The relationship between these metrics can be summarized as follows:\n",
    "\n",
    "High homogeneity means that clusters are pure and contain data points from a single true class.\n",
    "High completeness means that all data points of a given true class are assigned to the same cluster.\n",
    "The V-measure combines both aspects and provides an overall measure of clustering quality that considers both homogeneity and completeness.\n",
    "For the same clustering result, homogeneity and completeness can have different values, and their balance can vary. The V-measure provides a way to balance and assess the trade-off between these two aspects. In practice, the goal is to achieve a high V-measure, indicating that clusters are both internally pure and externally well-matched to the true class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d92bae3-48ed-408c-95f4-c37c9d9d43b4",
   "metadata": {},
   "source": [
    "Q10. How can the Silhouette Coefficient be used to compare the quality of different clustering algorithms\n",
    "on the same dataset? What are some potential issues to watch out for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09089dc-8655-4cec-9847-813175719f45",
   "metadata": {},
   "source": [
    "The Silhouette Coefficient can be used to compare the quality of different clustering algorithms on the same dataset by providing a measure of how similar each data point is to its own cluster compared to other clusters. This allows you to assess the overall quality and consistency of clusters produced by different algorithms.\n",
    "\n",
    "Steps to compare the quality of different clustering algorithms using the Silhouette Coefficient:\n",
    "\n",
    "Apply multiple clustering algorithms to the dataset, producing different sets of clusters.\n",
    "Calculate the Silhouette Coefficient for each clustering result.\n",
    "Compare the Silhouette Coefficients obtained from different algorithms.\n",
    "The algorithm with the highest average Silhouette Coefficient is likely to produce better-defined and more consistent clusters.\n",
    "Drawbacks of Silhouette Coefficient\n",
    "\n",
    "Different Algorithms, Different Results: Different clustering algorithms have different assumptions and characteristics, and they may produce varying types of clusters. A high Silhouette Coefficient doesn't necessarily mean that one algorithm is universally better than another. Consider whether the characteristics of the clusters align with your domain-specific goals.\n",
    "\n",
    "Sensitivity to Distance Metric: The Silhouette Coefficient's effectiveness can be influenced by the choice of distance metric. Different clustering algorithms might be more suited to specific distance metrics, which could affect the comparison. Ensure that you are using consistent distance metrics when comparing algorithms.\n",
    "\n",
    "Interpretability: The Silhouette Coefficient provides a numeric score but doesn't provide insights into the interpretability of the clusters. Clusters that achieve a high Silhouette Coefficient might not be semantically meaningful or useful for your specific problem.\n",
    "\n",
    "Dependence on Parameters: Some clustering algorithms have hyperparameters that can impact the Silhouette Coefficient. You should perform parameter tuning for each algorithm to ensure a fair comparison.\n",
    "\n",
    "Data Preprocessing: Preprocessing steps like feature scaling or dimensionality reduction can affect the performance of clustering algorithms and, consequently, the Silhouette Coefficient. Ensure that preprocessing is consistent across all algorithms being compared.\n",
    "\n",
    "In summary, the Silhouette Coefficient is a valuable tool for comparing clustering algorithms, but it should be used alongside other evaluation metrics and domain knowledge to make informed decisions about which algorithm is most suitable for a specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0cb21-37f1-43d6-b507-0cda20d96232",
   "metadata": {},
   "source": [
    "Q11. How does the Davies-Bouldin Index measure the separation and compactness of clusters? What are\n",
    "some assumptions it makes about the data and the clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a3886a-9a60-4c19-bd5c-0ab6d2116627",
   "metadata": {},
   "source": [
    "The Davies-Bouldin Index measures the separation and compactness of clusters in the following way:\n",
    "Separation: For each cluster, it calculates the average dissimilarity (distance) between that cluster and the cluster it is most similar to among the other clusters. This represents how well-separated clusters are from each other.\n",
    "\n",
    "Compactness: For each cluster, it calculates the average dissimilarity of all points within that cluster. This represents how compact the points are within each cluster.\n",
    "\n",
    "The Davies-Bouldin Index then combines these two measures to assess the overall quality of clustering. Smaller index values indicate better separation and compactness, implying well-defined and well-separated clusters.\n",
    "\n",
    "Assumptions and considerations of the Davies-Bouldin Index:\n",
    "Assumption of Euclidean Distance: The index assumes that the distance metric used is Euclidean. If the data doesn't adhere to Euclidean geometry, the index may not be suitable.\n",
    "\n",
    "Assumption of Cluster Shape: It assumes that clusters have a roughly spherical or convex shape. If clusters are highly irregular or non-convex, the index may not accurately reflect their quality.\n",
    "\n",
    "Equal Cluster Sizes: The index assumes that clusters have approximately equal sizes. If clusters have highly imbalanced sizes, it might not work well.\n",
    "\n",
    "Assumption of Non-Overlapping Clusters: It assumes that clusters are non-overlapping. If clusters overlap significantly, the index may not provide meaningful results.\n",
    "\n",
    "Nearest Neighbor Clusters: The index pairs each cluster with its nearest neighbor. This may not account for more complex relationships between clusters.\n",
    "\n",
    "In summary, the Davies-Bouldin Index uses separation and compactness measures to assess clustering quality, but it makes certain assumptions about data and cluster characteristics that should be considered when interpreting its results. It is most suitable for datasets with roughly spherical or convex clusters and where the Euclidean distance metric is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e3143-0790-4ad8-a997-782894ed300d",
   "metadata": {},
   "source": [
    "Q12. Can the Silhouette Coefficient be used to evaluate hierarchical clustering algorithms? If so, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f64fb54-2c4e-49fc-8440-b193210a5fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
